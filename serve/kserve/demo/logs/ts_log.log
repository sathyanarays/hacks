2024-05-16T16:40:16,110 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-05-16T16:40:16,110 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-05-16T16:40:16,112 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-05-16T16:40:16,112 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-05-16T16:40:16,139 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml
2024-05-16T16:40:16,139 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml
2024-05-16T16:40:16,263 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.10.0
TS Home: /home/ssaravanamut/.local/lib/python3.10/site-packages
Current directory: /home/ssaravanamut/hacks/serve/kserve/demo
Temp directory: /tmp
Metrics config path: /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 32
Max heap size: 15996 M
Python executable: /usr/bin/python3
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ssaravanamut/hacks/serve/kserve/demo/storage-container/mnist_model/model-store
Initial Models: storage-container/mnist_model/model-store/mnist.mar
Log dir: /home/ssaravanamut/hacks/serve/kserve/demo/logs
Metrics dir: /home/ssaravanamut/hacks/serve/kserve/demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ssaravanamut/hacks/serve/kserve/demo/storage-container/mnist_model/model-store
CPP log config: N/A
Model config: N/A
System metrics command: default
2024-05-16T16:40:16,263 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.10.0
TS Home: /home/ssaravanamut/.local/lib/python3.10/site-packages
Current directory: /home/ssaravanamut/hacks/serve/kserve/demo
Temp directory: /tmp
Metrics config path: /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 32
Max heap size: 15996 M
Python executable: /usr/bin/python3
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ssaravanamut/hacks/serve/kserve/demo/storage-container/mnist_model/model-store
Initial Models: storage-container/mnist_model/model-store/mnist.mar
Log dir: /home/ssaravanamut/hacks/serve/kserve/demo/logs
Metrics dir: /home/ssaravanamut/hacks/serve/kserve/demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ssaravanamut/hacks/serve/kserve/demo/storage-container/mnist_model/model-store
CPP log config: N/A
Model config: N/A
System metrics command: default
2024-05-16T16:40:16,268 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-05-16T16:40:16,268 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-05-16T16:40:16,276 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: storage-container/mnist_model/model-store/mnist.mar
2024-05-16T16:40:16,276 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: storage-container/mnist_model/model-store/mnist.mar
2024-05-16T16:40:16,327 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2024-05-16T16:40:16,327 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2024-05-16T16:40:16,328 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2024-05-16T16:40:16,328 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2024-05-16T16:40:16,328 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2024-05-16T16:40:16,328 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2024-05-16T16:40:16,328 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 1
2024-05-16T16:40:16,328 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 1
2024-05-16T16:40:16,334 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:16,334 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:16,335 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2024-05-16T16:40:16,335 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2024-05-16T16:40:16,376 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2024-05-16T16:40:16,376 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2024-05-16T16:40:16,376 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2024-05-16T16:40:16,376 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2024-05-16T16:40:16,377 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2024-05-16T16:40:16,377 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2024-05-16T16:40:16,377 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2024-05-16T16:40:16,377 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2024-05-16T16:40:16,377 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2024-05-16T16:40:16,377 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2024-05-16T16:40:16,481 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2024-05-16T16:40:16,481 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2024-05-16T16:40:16,525 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:40:16,525 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:40:16,839 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=913195
2024-05-16T16:40:16,840 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:40:16,843 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:40:16,843 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]913195
2024-05-16T16:40:16,843 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:40:16,843 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:40:16,843 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change null -> WORKER_STARTED
2024-05-16T16:40:16,843 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change null -> WORKER_STARTED
2024-05-16T16:40:16,846 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:16,846 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:16,848 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:40:16,850 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857816850
2024-05-16T16:40:16,850 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857816850
2024-05-16T16:40:16,851 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857816851
2024-05-16T16:40:16,851 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857816851
2024-05-16T16:40:16,863 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:40:17,211 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:40:17,211 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:40:17,211 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:40:17,211 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:40:17,211 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:40:17,211 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:40:17,211 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:40:17,211 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:40:17,211 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:17,212 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:17,212 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:40:17,212 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:40:17,212 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:40:17,212 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:40:17,212 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:17,212 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/0801ff6755f34d89b85fe882886710b1/mnist_handler.py", line 2, in <module>
2024-05-16T16:40:17,212 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:40:17,212 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:17,212 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:40:17,212 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:40:17,212 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:40:17,212 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:40:17,213 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:17,213 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:40:17,213 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:40:17,214 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:40:17,214 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:40:17,214 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:17,214 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:17,214 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:40:17,214 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:40:17,214 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:17,214 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:17,214 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:40:17,214 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:40:17,213 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:17,213 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:17,229 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:17,229 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:17,229 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:17,229 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:17,230 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1715857817230
2024-05-16T16:40:17,230 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1715857817230
2024-05-16T16:40:17,230 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:17,230 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:17,230 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:17,230 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:17,230 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2024-05-16T16:40:17,230 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2024-05-16T16:40:17,238 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:17,238 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:17,238 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:40:17,238 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:40:18,232 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:18,232 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:18,743 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=913450
2024-05-16T16:40:18,743 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:40:18,746 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:40:18,746 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]913450
2024-05-16T16:40:18,746 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:40:18,746 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:40:18,746 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:40:18,746 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:40:18,746 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:18,746 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:18,747 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857818747
2024-05-16T16:40:18,747 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:40:18,747 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857818747
2024-05-16T16:40:18,747 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857818747
2024-05-16T16:40:18,747 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857818747
2024-05-16T16:40:18,755 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:40:19,099 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:40:19,099 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:40:19,099 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:40:19,099 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:40:19,099 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:40:19,099 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:40:19,099 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:40:19,099 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:40:19,099 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:19,099 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:19,099 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:40:19,099 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:40:19,099 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:40:19,099 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/0801ff6755f34d89b85fe882886710b1/mnist_handler.py", line 2, in <module>
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:40:19,100 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:40:19,100 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:40:19,100 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:19,100 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:40:19,100 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:40:19,100 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:40:19,100 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:40:19,100 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:40:19,100 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:40:19,100 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:19,100 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:40:19,100 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:40:19,100 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:40:19,101 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:40:19,101 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:40:19,101 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:19,101 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:19,101 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:19,101 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:19,101 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:40:19,101 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:19,101 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:19,101 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2024-05-16T16:40:19,101 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:40:19,101 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2024-05-16T16:40:19,101 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:19,101 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:19,110 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:40:19,110 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:40:20,102 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:20,102 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:20,602 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=913501
2024-05-16T16:40:20,603 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:40:20,606 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:40:20,606 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]913501
2024-05-16T16:40:20,606 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:40:20,606 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:40:20,606 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:40:20,606 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:40:20,606 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:20,606 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:20,607 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:40:20,607 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857820607
2024-05-16T16:40:20,607 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857820607
2024-05-16T16:40:20,607 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857820607
2024-05-16T16:40:20,607 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857820607
2024-05-16T16:40:20,615 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:40:20,958 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:40:20,959 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:40:20,959 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:20,959 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:20,959 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:20,959 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:20,959 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:40:20,959 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:20,959 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:20,960 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:40:20,960 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:20,960 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:40:20,960 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:20,960 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:40:20,960 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:20,960 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:20,960 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:40:20,960 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:40:20,960 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:40:20,960 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:40:20,960 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:20,960 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:20,960 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:20,960 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:40:20,960 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:40:20,960 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:40:20,960 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:20,960 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:20,960 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:20,960 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2024-05-16T16:40:20,960 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:40:20,960 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2024-05-16T16:40:20,960 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:20,960 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:20,969 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:40:20,969 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:40:22,961 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:22,961 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:23,497 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=913688
2024-05-16T16:40:23,497 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:40:23,501 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:40:23,501 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]913688
2024-05-16T16:40:23,501 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:40:23,501 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:40:23,501 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:40:23,501 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:40:23,501 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:23,501 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:23,502 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857823502
2024-05-16T16:40:23,502 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:40:23,502 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857823502
2024-05-16T16:40:23,502 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857823502
2024-05-16T16:40:23,502 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857823502
2024-05-16T16:40:23,510 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/0801ff6755f34d89b85fe882886710b1/mnist_handler.py", line 2, in <module>
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:40:23,856 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:23,857 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:40:23,857 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:23,857 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:23,857 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:40:23,857 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:40:23,857 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:23,857 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:23,857 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:23,857 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:23,857 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:23,857 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:23,857 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:40:23,857 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:40:23,858 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:23,858 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:23,858 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:23,858 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:23,858 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2024-05-16T16:40:23,858 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2024-05-16T16:40:23,866 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:23,866 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:23,866 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:40:23,866 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:40:26,859 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:26,859 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:27,376 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=913883
2024-05-16T16:40:27,377 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:40:27,380 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:40:27,380 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]913883
2024-05-16T16:40:27,380 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:40:27,380 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:40:27,380 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:40:27,380 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:40:27,380 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:27,380 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:27,381 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857827381
2024-05-16T16:40:27,381 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857827381
2024-05-16T16:40:27,381 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857827381
2024-05-16T16:40:27,381 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857827381
2024-05-16T16:40:27,381 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:40:27,389 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/0801ff6755f34d89b85fe882886710b1/mnist_handler.py", line 2, in <module>
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:40:27,732 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:40:27,732 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:27,732 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:27,733 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:27,733 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:27,733 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:27,733 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:27,733 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:27,733 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:27,733 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:27,733 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:27,733 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:40:27,733 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:40:27,733 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:27,733 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:27,733 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:27,733 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:27,733 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2024-05-16T16:40:27,733 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2024-05-16T16:40:27,741 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:27,741 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:27,741 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:40:27,741 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:40:32,734 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:32,734 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:33,264 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=914113
2024-05-16T16:40:33,265 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:40:33,268 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:40:33,268 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]914113
2024-05-16T16:40:33,268 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:40:33,268 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:40:33,268 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:40:33,268 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:40:33,268 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:33,268 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:33,269 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:40:33,269 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857833269
2024-05-16T16:40:33,269 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857833269
2024-05-16T16:40:33,269 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857833269
2024-05-16T16:40:33,269 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857833269
2024-05-16T16:40:33,277 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/0801ff6755f34d89b85fe882886710b1/mnist_handler.py", line 2, in <module>
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:40:33,627 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:40:33,628 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:33,628 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:33,628 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:33,628 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:33,628 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:33,628 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:33,628 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:33,628 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:33,628 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:33,628 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:33,628 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:40:33,628 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:40:33,628 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:33,628 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:33,628 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:33,628 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:33,628 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2024-05-16T16:40:33,628 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2024-05-16T16:40:33,636 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:33,636 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:33,636 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:40:33,636 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:40:40,417 [INFO ] epollEventLoopGroup-4-1 ACCESS_LOG - /127.0.0.1:46318 "GET / HTTP/1.1" 405 1
2024-05-16T16:40:40,427 [INFO ] epollEventLoopGroup-4-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:99b2c4d-lcedt,timestamp:1715857840
2024-05-16T16:40:41,629 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:41,629 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:42,129 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=914626
2024-05-16T16:40:42,129 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:40:42,132 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:40:42,132 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]914626
2024-05-16T16:40:42,132 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:40:42,132 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:40:42,132 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:40:42,132 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:40:42,132 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:42,132 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:42,133 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857842133
2024-05-16T16:40:42,133 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857842133
2024-05-16T16:40:42,133 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:40:42,134 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857842133
2024-05-16T16:40:42,134 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857842133
2024-05-16T16:40:42,141 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/0801ff6755f34d89b85fe882886710b1/mnist_handler.py", line 2, in <module>
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:40:42,488 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:40:42,488 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:42,488 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:42,489 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:42,489 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:42,489 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:42,489 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:42,489 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:42,489 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:42,489 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:42,489 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:42,489 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:40:42,489 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:40:42,489 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:42,489 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:42,489 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:42,489 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:42,489 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2024-05-16T16:40:42,489 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2024-05-16T16:40:42,498 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:42,498 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:42,498 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:40:42,498 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:40:46,786 [INFO ] epollEventLoopGroup-4-1 ACCESS_LOG - /127.0.0.1:46328 "GET / HTTP/1.1" 405 0
2024-05-16T16:40:46,786 [INFO ] epollEventLoopGroup-4-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:99b2c4d-lcedt,timestamp:1715857846
2024-05-16T16:40:55,490 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:55,490 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:40:55,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=915241
2024-05-16T16:40:55,995 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:40:55,998 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:40:55,998 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]915241
2024-05-16T16:40:55,998 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:40:55,998 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:40:55,998 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:40:55,998 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:40:55,998 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:55,998 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:40:55,999 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857855999
2024-05-16T16:40:55,999 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857855999
2024-05-16T16:40:55,999 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:40:55,999 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857855999
2024-05-16T16:40:55,999 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857855999
2024-05-16T16:40:56,006 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/0801ff6755f34d89b85fe882886710b1/mnist_handler.py", line 2, in <module>
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:40:56,359 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:40:56,360 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:40:56,360 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:40:56,360 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:40:56,360 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:40:56,360 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:40:56,360 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:56,360 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:56,360 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:56,360 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:40:56,360 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:40:56,360 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:40:56,360 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:40:56,360 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:40:56,360 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:40:56,360 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:40:56,360 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:56,360 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:40:56,360 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:56,360 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:40:56,360 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:56,360 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:40:56,360 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:56,360 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:40:56,360 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:40:56,360 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:40:56,360 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:56,360 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:40:56,360 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:56,360 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:40:56,360 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2024-05-16T16:40:56,360 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2024-05-16T16:40:56,369 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:56,369 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:40:56,369 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:40:56,369 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:41:16,530 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:41:16,530 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:41:17,361 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:41:17,361 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:41:17,869 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=916288
2024-05-16T16:41:17,869 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:41:17,872 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:41:17,872 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]916288
2024-05-16T16:41:17,872 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:41:17,872 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:41:17,872 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:41:17,872 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:41:17,872 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:41:17,872 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:41:17,873 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:41:17,873 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857877873
2024-05-16T16:41:17,873 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857877873
2024-05-16T16:41:17,874 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857877874
2024-05-16T16:41:17,874 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857877874
2024-05-16T16:41:17,881 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/0801ff6755f34d89b85fe882886710b1/mnist_handler.py", line 2, in <module>
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:41:18,224 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:41:18,224 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:41:18,224 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:41:18,224 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:41:18,224 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:41:18,224 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:41:18,224 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:41:18,225 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:41:18,225 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:41:18,225 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:41:18,225 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:41:18,225 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:41:18,225 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:41:18,225 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:41:18,225 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:41:18,225 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:41:18,225 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:41:18,225 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2024-05-16T16:41:18,225 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2024-05-16T16:41:18,233 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:41:18,233 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:41:18,233 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:41:18,233 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:41:52,226 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:41:52,226 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:41:52,754 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=917945
2024-05-16T16:41:52,754 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:41:52,758 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:41:52,758 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]917945
2024-05-16T16:41:52,758 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:41:52,758 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:41:52,758 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:41:52,758 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:41:52,758 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:41:52,758 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:41:52,758 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:41:52,759 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857912759
2024-05-16T16:41:52,759 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857912759
2024-05-16T16:41:52,759 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857912759
2024-05-16T16:41:52,759 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857912759
2024-05-16T16:41:52,766 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/0801ff6755f34d89b85fe882886710b1/mnist_handler.py", line 2, in <module>
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:41:53,110 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:41:53,111 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:41:53,111 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:41:53,111 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:41:53,111 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:41:53,111 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:41:53,111 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:41:53,111 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:41:53,111 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:41:53,111 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:41:53,111 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:41:53,111 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:41:53,111 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:41:53,111 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:41:53,111 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:41:53,111 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:41:53,111 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:41:53,111 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:41:53,111 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:41:53,111 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:41:53,111 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:41:53,111 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:41:53,111 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:41:53,111 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:41:53,111 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2024-05-16T16:41:53,111 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2024-05-16T16:41:53,120 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:41:53,120 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:41:53,120 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:41:53,120 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:42:12,855 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /127.0.0.1:43668 "PUT /predictions/densenet161 HTTP/1.1" 404 2
2024-05-16T16:42:12,855 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:99b2c4d-lcedt,timestamp:1715857932
2024-05-16T16:42:16,537 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:42:16,537 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:42:44,429 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /127.0.0.1:45054 "GET /predictions/mnist-T HTTP/1.1" 404 0
2024-05-16T16:42:44,429 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:99b2c4d-lcedt,timestamp:1715857964
2024-05-16T16:42:48,112 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:42:48,112 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:42:48,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=920443
2024-05-16T16:42:48,640 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:42:48,643 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:42:48,643 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]920443
2024-05-16T16:42:48,643 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:42:48,643 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:42:48,643 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:42:48,643 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:42:48,643 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:42:48,643 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:42:48,644 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857968644
2024-05-16T16:42:48,644 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:42:48,644 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715857968644
2024-05-16T16:42:48,644 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857968644
2024-05-16T16:42:48,644 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715857968644
2024-05-16T16:42:48,652 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:42:48,993 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:42:48,993 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/0801ff6755f34d89b85fe882886710b1/mnist_handler.py", line 2, in <module>
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:42:48,994 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:42:48,994 [INFO ] epollEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:42:48,994 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:42:48,994 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:42:48,994 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:42:48,994 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:42:48,995 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:42:48,994 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:42:48,995 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:42:48,995 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:42:48,995 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:42:48,995 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:42:48,995 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:42:48,995 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:42:48,995 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:42:48,995 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:42:48,995 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:42:48,995 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:42:48,995 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:42:48,995 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:42:48,995 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:42:48,995 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:42:48,995 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2024-05-16T16:42:48,995 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2024-05-16T16:42:49,003 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:42:49,003 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:42:49,003 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:42:49,003 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:42:51,500 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:mnist,model_version:default|#hostname:99b2c4d-lcedt,timestamp:1715857971
2024-05-16T16:43:04,479 [INFO ] epollEventLoopGroup-3-4 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:mnist,model_version:default|#hostname:99b2c4d-lcedt,timestamp:1715857984
2024-05-16T16:43:16,542 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:43:16,542 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:44:16,532 [ERROR] Thread-5 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:44:16,532 [ERROR] Thread-5 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:44:17,996 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:44:17,996 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:44:18,511 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=924658
2024-05-16T16:44:18,511 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:44:18,515 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:44:18,515 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]924658
2024-05-16T16:44:18,515 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:44:18,515 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:44:18,515 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:44:18,515 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:44:18,515 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:44:18,515 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:44:18,516 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:44:18,516 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858058516
2024-05-16T16:44:18,516 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858058516
2024-05-16T16:44:18,516 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858058516
2024-05-16T16:44:18,516 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858058516
2024-05-16T16:44:18,523 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/0801ff6755f34d89b85fe882886710b1/mnist_handler.py", line 2, in <module>
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:44:18,868 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:44:18,869 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:44:18,869 [INFO ] epollEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:44:18,869 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:44:18,869 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:44:18,869 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:44:18,869 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:44:18,869 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:44:18,869 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:44:18,869 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:44:18,869 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:44:18,869 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:44:18,869 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:44:18,869 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:44:18,869 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:44:18,869 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:44:18,869 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:44:18,869 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2024-05-16T16:44:18,869 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.
2024-05-16T16:44:18,879 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:44:18,879 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:44:18,879 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:44:18,879 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:45:16,531 [ERROR] Thread-6 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:45:16,531 [ERROR] Thread-6 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:45:31,910 [INFO ] epollEventLoopGroup-3-5 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:mnist,model_version:default|#hostname:99b2c4d-lcedt,timestamp:1715858131
2024-05-16T16:46:16,530 [ERROR] Thread-7 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:46:16,530 [ERROR] Thread-7 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:46:42,871 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:46:42,871 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:46:43,369 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=930799
2024-05-16T16:46:43,369 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:46:43,372 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:46:43,372 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]930799
2024-05-16T16:46:43,372 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:46:43,372 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:46:43,372 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:46:43,372 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:46:43,372 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:46:43,372 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:46:43,373 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858203373
2024-05-16T16:46:43,373 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858203373
2024-05-16T16:46:43,373 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858203373
2024-05-16T16:46:43,373 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858203373
2024-05-16T16:46:43,373 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:46:43,380 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/0801ff6755f34d89b85fe882886710b1/mnist_handler.py", line 2, in <module>
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:46:43,729 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:46:43,729 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:46:43,729 [INFO ] epollEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:46:43,730 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:46:43,730 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:46:43,730 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:46:43,730 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:46:43,730 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:46:43,730 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:46:43,730 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:46:43,730 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:46:43,730 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:46:43,730 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:46:43,730 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:46:43,730 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:46:43,730 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:46:43,730 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:46:43,738 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:46:43,738 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:46:43,738 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:46:43,738 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:46:51,259 [INFO ] epollEventLoopGroup-3-6 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:mnist,model_version:default|#hostname:99b2c4d-lcedt,timestamp:1715858211
2024-05-16T16:47:17,739 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-05-16T16:47:17,739 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-05-16T16:47:17,740 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-05-16T16:47:17,740 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-05-16T16:47:17,762 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml
2024-05-16T16:47:17,762 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml
2024-05-16T16:47:17,924 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.10.0
TS Home: /home/ssaravanamut/.local/lib/python3.10/site-packages
Current directory: /home/ssaravanamut/hacks/serve/kserve/demo
Temp directory: /tmp
Metrics config path: /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 32
Max heap size: 15996 M
Python executable: /usr/bin/python3
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ssaravanamut/hacks/serve/kserve/demo/storage-container/mnist_model/model-store
Initial Models: storage-container/mnist_model/model-store/mnist.mar
Log dir: /home/ssaravanamut/hacks/serve/kserve/demo/logs
Metrics dir: /home/ssaravanamut/hacks/serve/kserve/demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ssaravanamut/hacks/serve/kserve/demo/storage-container/mnist_model/model-store
CPP log config: N/A
Model config: N/A
System metrics command: default
2024-05-16T16:47:17,924 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.10.0
TS Home: /home/ssaravanamut/.local/lib/python3.10/site-packages
Current directory: /home/ssaravanamut/hacks/serve/kserve/demo
Temp directory: /tmp
Metrics config path: /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 32
Max heap size: 15996 M
Python executable: /usr/bin/python3
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/ssaravanamut/hacks/serve/kserve/demo/storage-container/mnist_model/model-store
Initial Models: storage-container/mnist_model/model-store/mnist.mar
Log dir: /home/ssaravanamut/hacks/serve/kserve/demo/logs
Metrics dir: /home/ssaravanamut/hacks/serve/kserve/demo/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ssaravanamut/hacks/serve/kserve/demo/storage-container/mnist_model/model-store
CPP log config: N/A
Model config: N/A
System metrics command: default
2024-05-16T16:47:17,929 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-05-16T16:47:17,929 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-05-16T16:47:17,936 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: storage-container/mnist_model/model-store/mnist.mar
2024-05-16T16:47:17,936 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: storage-container/mnist_model/model-store/mnist.mar
2024-05-16T16:47:17,986 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2024-05-16T16:47:17,986 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mnist
2024-05-16T16:47:17,986 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2024-05-16T16:47:17,986 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mnist
2024-05-16T16:47:17,986 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2024-05-16T16:47:17,986 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mnist loaded.
2024-05-16T16:47:17,987 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 1
2024-05-16T16:47:17,987 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mnist, count: 1
2024-05-16T16:47:17,993 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:17,993 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:17,994 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2024-05-16T16:47:17,994 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2024-05-16T16:47:18,033 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2024-05-16T16:47:18,033 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2024-05-16T16:47:18,033 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2024-05-16T16:47:18,033 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2024-05-16T16:47:18,034 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2024-05-16T16:47:18,034 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2024-05-16T16:47:18,034 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2024-05-16T16:47:18,034 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2024-05-16T16:47:18,034 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2024-05-16T16:47:18,034 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2024-05-16T16:47:18,162 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2024-05-16T16:47:18,162 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2024-05-16T16:47:18,206 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:47:18,206 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:47:18,502 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=932817
2024-05-16T16:47:18,502 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:47:18,505 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:47:18,505 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]932817
2024-05-16T16:47:18,505 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:47:18,505 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:47:18,506 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change null -> WORKER_STARTED
2024-05-16T16:47:18,506 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change null -> WORKER_STARTED
2024-05-16T16:47:18,509 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:18,509 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:18,512 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:47:18,513 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858238513
2024-05-16T16:47:18,513 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858238513
2024-05-16T16:47:18,514 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858238514
2024-05-16T16:47:18,514 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858238514
2024-05-16T16:47:18,526 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:47:18,873 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:47:18,873 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:47:18,873 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:47:18,873 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:47:18,873 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:47:18,873 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:47:18,874 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:18,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:47:18,874 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:18,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:47:18,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:18,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:18,874 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:18,874 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:18,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:47:18,875 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:47:18,875 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:47:18,875 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:47:18,875 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/24f51cd63d0c4bc5a16c30c6a43c412e/mnist_handler.py", line 2, in <module>
2024-05-16T16:47:18,875 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:47:18,875 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:47:18,875 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:47:18,875 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:47:18,875 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:47:18,875 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:47:18,876 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:47:18,876 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:47:18,876 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:47:18,876 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:47:18,876 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:47:18,876 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:47:18,876 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:47:18,876 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:47:18,876 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:47:18,876 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:47:18,877 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:47:18,877 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:47:18,877 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:47:18,877 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:47:18,877 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:47:18,877 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:47:18,877 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:47:18,877 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:47:18,877 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:18,877 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:18,877 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:47:18,878 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:47:18,878 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:18,878 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:18,878 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:47:18,878 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:47:18,875 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:18,875 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:18,880 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:18,880 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:18,880 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:18,880 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:18,880 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1715858238880
2024-05-16T16:47:18,880 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1715858238880
2024-05-16T16:47:18,880 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:18,880 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:18,880 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:18,880 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:18,880 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2024-05-16T16:47:18,880 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2024-05-16T16:47:18,889 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:18,889 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:18,889 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:47:18,889 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:47:19,881 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:19,881 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:20,391 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=932898
2024-05-16T16:47:20,392 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:47:20,395 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:47:20,395 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]932898
2024-05-16T16:47:20,395 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:47:20,395 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:47:20,395 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:47:20,395 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:47:20,395 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:20,395 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:20,396 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:47:20,396 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858240396
2024-05-16T16:47:20,396 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858240396
2024-05-16T16:47:20,396 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858240396
2024-05-16T16:47:20,396 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858240396
2024-05-16T16:47:20,404 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:47:20,748 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/24f51cd63d0c4bc5a16c30c6a43c412e/mnist_handler.py", line 2, in <module>
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:47:20,749 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:47:20,749 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:47:20,749 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:47:20,749 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:20,749 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:47:20,750 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:47:20,749 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:20,750 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:47:20,749 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:20,750 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:47:20,750 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:47:20,750 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:20,750 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:20,750 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:47:20,750 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:20,750 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:20,750 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:47:20,750 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:47:20,750 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:47:20,750 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:47:20,750 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:20,750 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:20,750 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:47:20,750 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:47:20,750 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:20,750 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:20,750 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:47:20,750 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2024-05-16T16:47:20,750 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2024-05-16T16:47:20,750 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:20,750 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:20,759 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:47:20,759 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:47:21,751 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:21,751 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:22,276 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=933036
2024-05-16T16:47:22,276 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:47:22,280 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:47:22,280 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]933036
2024-05-16T16:47:22,280 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:47:22,280 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:47:22,280 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:47:22,280 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:47:22,280 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:22,280 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:22,280 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:47:22,280 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858242280
2024-05-16T16:47:22,280 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858242280
2024-05-16T16:47:22,280 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858242280
2024-05-16T16:47:22,280 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858242280
2024-05-16T16:47:22,288 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:47:22,633 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:47:22,633 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:47:22,633 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:47:22,633 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:47:22,633 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:47:22,633 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:47:22,633 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:47:22,633 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:47:22,633 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:22,633 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:22,633 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:22,633 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:47:22,633 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:22,634 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:47:22,634 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:47:22,634 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:22,634 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:47:22,634 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:22,634 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/24f51cd63d0c4bc5a16c30c6a43c412e/mnist_handler.py", line 2, in <module>
2024-05-16T16:47:22,634 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:22,634 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:47:22,634 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:22,634 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:22,634 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:22,634 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:22,634 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:47:22,634 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:22,634 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:47:22,634 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:47:22,634 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:47:22,634 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:22,634 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:47:22,634 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:22,634 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:22,634 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:22,634 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:47:22,634 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:22,634 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:22,635 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2024-05-16T16:47:22,635 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2024-05-16T16:47:22,643 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:47:22,643 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:47:24,635 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:24,635 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:25,139 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=933232
2024-05-16T16:47:25,140 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:47:25,143 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:47:25,143 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]933232
2024-05-16T16:47:25,143 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:47:25,143 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:47:25,143 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:47:25,143 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:47:25,143 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:25,143 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:25,144 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:47:25,144 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858245144
2024-05-16T16:47:25,144 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858245144
2024-05-16T16:47:25,144 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858245144
2024-05-16T16:47:25,144 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858245144
2024-05-16T16:47:25,151 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:47:25,497 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/24f51cd63d0c4bc5a16c30c6a43c412e/mnist_handler.py", line 2, in <module>
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:47:25,498 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:47:25,498 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:47:25,498 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:25,498 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:47:25,498 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:25,498 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:25,498 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:47:25,498 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:25,498 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:25,499 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:25,499 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:25,499 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:47:25,499 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:47:25,499 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:47:25,499 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:25,499 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:25,499 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:25,499 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:25,499 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:25,499 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:25,499 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2024-05-16T16:47:25,499 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2024-05-16T16:47:25,499 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:47:25,499 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:25,499 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:25,508 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:47:25,508 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:47:27,417 [INFO ] epollEventLoopGroup-4-1 ACCESS_LOG - /127.0.0.1:36962 "PUT /predictions/mnist HTTP/1.1" 404 2
2024-05-16T16:47:27,418 [INFO ] epollEventLoopGroup-4-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:99b2c4d-lcedt,timestamp:1715858247
2024-05-16T16:47:28,500 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:28,500 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:29,003 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=933493
2024-05-16T16:47:29,003 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:47:29,006 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:47:29,006 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]933493
2024-05-16T16:47:29,006 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:47:29,006 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:47:29,007 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:47:29,007 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:47:29,007 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:29,007 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:29,007 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858249007
2024-05-16T16:47:29,007 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858249007
2024-05-16T16:47:29,007 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858249007
2024-05-16T16:47:29,007 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858249007
2024-05-16T16:47:29,007 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:47:29,015 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/24f51cd63d0c4bc5a16c30c6a43c412e/mnist_handler.py", line 2, in <module>
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:47:29,362 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:29,362 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:47:29,362 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:47:29,363 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:29,363 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:29,363 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:29,363 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:29,363 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:47:29,363 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:47:29,363 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:29,363 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:29,363 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:29,363 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:29,363 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:29,363 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:29,363 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:47:29,363 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:47:29,363 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:29,363 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:29,363 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:29,363 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:29,363 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2024-05-16T16:47:29,363 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2024-05-16T16:47:29,372 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:29,372 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:29,372 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:47:29,372 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:47:34,364 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:34,364 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:34,880 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=933671
2024-05-16T16:47:34,880 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:47:34,883 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:47:34,883 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]933671
2024-05-16T16:47:34,884 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:47:34,884 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:47:34,884 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:47:34,884 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:47:34,884 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:34,884 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:34,884 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:47:34,884 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858254884
2024-05-16T16:47:34,884 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858254884
2024-05-16T16:47:34,884 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858254884
2024-05-16T16:47:34,884 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858254884
2024-05-16T16:47:34,892 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/24f51cd63d0c4bc5a16c30c6a43c412e/mnist_handler.py", line 2, in <module>
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:47:35,239 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:35,240 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:35,240 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:47:35,240 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:47:35,239 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:35,240 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:35,240 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:35,240 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:47:35,239 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:35,240 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:47:35,240 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:35,240 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:35,240 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:35,240 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:35,240 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:35,240 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:35,240 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:35,240 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:35,240 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:47:35,240 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:47:35,240 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:35,240 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:35,240 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:35,240 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:35,240 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2024-05-16T16:47:35,240 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2024-05-16T16:47:35,251 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:35,251 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:35,251 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:47:35,251 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:47:38,392 [INFO ] epollEventLoopGroup-4-1 ACCESS_LOG - /127.0.0.1:45980 "PUT /predictions/mnist HTTP/1.1" 404 1
2024-05-16T16:47:38,392 [INFO ] epollEventLoopGroup-4-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:99b2c4d-lcedt,timestamp:1715858258
2024-05-16T16:47:39,419 [INFO ] epollEventLoopGroup-4-1 ACCESS_LOG - /127.0.0.1:45986 "PUT /predictions/mnist HTTP/1.1" 404 0
2024-05-16T16:47:39,419 [INFO ] epollEventLoopGroup-4-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:99b2c4d-lcedt,timestamp:1715858259
2024-05-16T16:47:43,241 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:43,241 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:43,766 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=934311
2024-05-16T16:47:43,766 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:47:43,769 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:47:43,769 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]934311
2024-05-16T16:47:43,769 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:47:43,769 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:47:43,769 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:47:43,769 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:47:43,769 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:43,769 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:43,770 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:47:43,770 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858263770
2024-05-16T16:47:43,770 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858263770
2024-05-16T16:47:43,770 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858263770
2024-05-16T16:47:43,770 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858263770
2024-05-16T16:47:43,778 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/24f51cd63d0c4bc5a16c30c6a43c412e/mnist_handler.py", line 2, in <module>
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:47:44,125 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:47:44,126 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:47:44,126 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:44,126 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:44,126 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:44,126 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:44,126 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:44,126 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:44,126 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:44,126 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:44,126 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:44,126 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:44,126 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:47:44,126 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:47:44,126 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:44,126 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:44,126 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:44,126 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:44,127 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2024-05-16T16:47:44,127 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2024-05-16T16:47:44,136 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:44,136 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:44,136 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:47:44,136 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:47:48,696 [INFO ] epollEventLoopGroup-4-1 ACCESS_LOG - /127.0.0.1:41174 "PUT /predictions/mnist HTTP/1.1" 404 0
2024-05-16T16:47:48,696 [INFO ] epollEventLoopGroup-4-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:99b2c4d-lcedt,timestamp:1715858268
2024-05-16T16:47:57,127 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:57,127 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:47:57,641 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=935119
2024-05-16T16:47:57,641 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:47:57,644 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:47:57,644 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]935119
2024-05-16T16:47:57,644 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:47:57,644 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:47:57,644 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:47:57,644 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:47:57,644 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:57,644 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:47:57,646 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858277646
2024-05-16T16:47:57,646 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858277646
2024-05-16T16:47:57,646 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858277646
2024-05-16T16:47:57,646 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858277646
2024-05-16T16:47:57,646 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:47:57,653 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:47:57,996 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:47:57,996 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:47:57,996 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:47:57,996 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:47:57,996 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/24f51cd63d0c4bc5a16c30c6a43c412e/mnist_handler.py", line 2, in <module>
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:47:57,997 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:47:57,997 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:47:57,997 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:47:57,997 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:47:57,997 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:47:57,997 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:57,997 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:47:57,997 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:57,997 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:47:57,997 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:57,997 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:47:57,997 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:47:57,997 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:47:57,998 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:57,998 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:47:57,998 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:57,998 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:47:57,998 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2024-05-16T16:47:57,998 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2024-05-16T16:47:58,006 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:58,006 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:47:58,006 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:47:58,006 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:48:18,222 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:48:18,222 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2024-05-16T16:48:18,998 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:48:18,998 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:48:19,517 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=936094
2024-05-16T16:48:19,517 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:48:19,520 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:48:19,520 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]936094
2024-05-16T16:48:19,520 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:48:19,520 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:48:19,520 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:48:19,520 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:48:19,520 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:48:19,520 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:48:19,521 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858299521
2024-05-16T16:48:19,521 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858299521
2024-05-16T16:48:19,521 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858299521
2024-05-16T16:48:19,521 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858299521
2024-05-16T16:48:19,521 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:48:19,529 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/24f51cd63d0c4bc5a16c30c6a43c412e/mnist_handler.py", line 2, in <module>
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:48:19,874 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:48:19,874 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:48:19,874 [INFO ] epollEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:48:19,874 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:48:19,874 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:48:19,874 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:48:19,874 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:48:19,875 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:48:19,875 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:48:19,875 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:48:19,875 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:48:19,875 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:48:19,875 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:48:19,875 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:48:19,875 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:48:19,875 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:48:19,875 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:48:19,875 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2024-05-16T16:48:19,875 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2024-05-16T16:48:19,884 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:48:19,884 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:48:19,884 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:48:19,884 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:48:53,876 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:48:53,876 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-05-16T16:48:54,420 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=938191
2024-05-16T16:48:54,421 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-05-16T16:48:54,424 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Successfully loaded /home/ssaravanamut/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-05-16T16:48:54,424 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - [PID]938191
2024-05-16T16:48:54,424 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Torch worker started.
2024-05-16T16:48:54,424 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-05-16T16:48:54,424 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:48:54,424 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2024-05-16T16:48:54,424 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:48:54,424 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-05-16T16:48:54,425 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-05-16T16:48:54,425 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858334425
2024-05-16T16:48:54,425 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1715858334425
2024-05-16T16:48:54,425 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858334425
2024-05-16T16:48:54,425 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1715858334425
2024-05-16T16:48:54,432 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - model_name: mnist, batchSize: 1
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Backend worker process died.
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 108, in load
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module, function_name = self._load_handler_file(handler)
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 153, in _load_handler_file
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name)
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap_external>", line 883, in exec_module
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/tmp/models/24f51cd63d0c4bc5a16c30c6a43c412e/mnist_handler.py", line 2, in <module>
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from ts.torch_handler.image_classifier import ImageClassifier
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/image_classifier.py", line 11, in <module>
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from .vision_handler import VisionHandler
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/torch_handler/vision_handler.py", line 11, in <module>
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     from captum.attr import IntegratedGradients
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'captum'
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - During handling of the above exception, another exception occurred:
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - 
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 263, in <module>
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     worker.run_server()
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 231, in run_server
2024-05-16T16:48:54,796 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 194, in handle_connection
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_service_worker.py", line 131, in load_model
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     service = model_loader.load(
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 110, in load
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = self._load_default_handler(handler)
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/home/ssaravanamut/.local/lib/python3.10/site-packages/ts/model_loader.py", line 159, in _load_default_handler
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     module = importlib.import_module(module_name, "ts.torch_handler")
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG -   File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
2024-05-16T16:48:54,797 [INFO ] W-9000-mnist_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 'ts.torch_handler.mnist_handler'
2024-05-16T16:48:54,798 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:48:54,798 [INFO ] epollEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2024-05-16T16:48:54,798 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:48:54,798 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2024-05-16T16:48:54,798 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:48:54,798 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., responseTimeout:120sec
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:229) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2024-05-16T16:48:54,798 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:48:54,798 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mnist, error: Worker died.
2024-05-16T16:48:54,798 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:48:54,798 [DEBUG] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mnist_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2024-05-16T16:48:54,798 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:48:54,798 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2024-05-16T16:48:54,798 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:48:54,798 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stderr
2024-05-16T16:48:54,798 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:48:54,798 [WARN ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mnist_1.0-stdout
2024-05-16T16:48:54,798 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2024-05-16T16:48:54,798 [INFO ] W-9000-mnist_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2024-05-16T16:48:54,806 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:48:54,806 [INFO ] W-9000-mnist_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stdout
2024-05-16T16:48:54,806 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
2024-05-16T16:48:54,806 [INFO ] W-9000-mnist_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mnist_1.0-stderr
